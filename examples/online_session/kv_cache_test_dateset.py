#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆ LMCache æ•°æ®é›†æµ‹è¯•
- ä»…æ”¯æŒæœ¬åœ° vLLM å®ä¾‹ (http://localhost:8000/v1)
- ä»…æ”¯æŒä»æœ¬æœºç¼“å­˜æ–‡ä»¶åŠ è½½æ•°æ®é›† (.json æˆ– .txt)
"""

import requests
import json
import time
import argparse
from pathlib import Path
from typing import List, Dict, Tuple


def load_dataset_from_file(file_path: str) -> List[str]:
    """ä»æœ¬åœ°æ–‡ä»¶åŠ è½½æ•°æ®é›† (.json æˆ– .txt)"""
    with open(file_path, 'r', encoding='utf-8') as f:
        if file_path.endswith('.json'):
            data = json.load(f)
            if isinstance(data, list):
                return data
            elif isinstance(data, dict) and 'prompts' in data:
                return data['prompts']
            else:
                raise ValueError("JSON æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®")
        elif file_path.endswith('.txt'):
            return [line.strip() for line in f if line.strip()]
        else:
            raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä½¿ç”¨ .json æˆ– .txt æ–‡ä»¶")


def measure_ttft(base_url: str, model: str, prompt: str, tag: str) -> float:
    """æµ‹é‡ TTFT"""
    print(f"ğŸ“¤ {tag}...")
    start = time.perf_counter()
    response = requests.post(
        f"{base_url}/completions",
        json={
            "model": model,
            "prompt": prompt,
            "temperature": 0.0,
            "max_tokens": 20,
            "stream": False
        },
        headers={"Content-Type": "application/json"},
        timeout=60
    )
    response.raise_for_status()
    end = time.perf_counter()
    ttft = end - start
    print(f"ğŸ• TTFT = {ttft:.3f}s")
    return ttft


def test_dataset(base_url: str, model: str, dataset: List[str]):
    results = []

    for i, prompt in enumerate(dataset, 1):
        print(f"\nğŸ“ Prompt {i}/{len(dataset)} (é•¿åº¦ {len(prompt)} å­—ç¬¦)")

        # å†·å¯åŠ¨
        ttft_cold = measure_ttft(base_url, model, prompt, "å†·å¯åŠ¨")
        # ç¬¬ä¸€æ¬¡ç¼“å­˜å¤ç”¨
        ttft_cached1 = measure_ttft(base_url, model, prompt, "ç¬¬ä¸€æ¬¡ç¼“å­˜å¤ç”¨")
        # ç¬¬äºŒæ¬¡ç¼“å­˜å¤ç”¨
        ttft_cached2 = measure_ttft(base_url, model, prompt, "ç¬¬äºŒæ¬¡ç¼“å­˜å¤ç”¨")

        print(f"ğŸš€ åŠ é€Ÿæ¯”: {ttft_cold/ttft_cached1:.2f}x, {ttft_cold/ttft_cached2:.2f}x")

        results.append((ttft_cold, ttft_cached1, ttft_cached2))

        time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«

    # ç»Ÿè®¡å¹³å‡
    avg_cold = sum(r[0] for r in results) / len(results)
    avg_cached1 = sum(r[1] for r in results) / len(results)
    avg_cached2 = sum(r[2] for r in results) / len(results)

    print("\nğŸ¯ æµ‹è¯•ç»“æœæ±‡æ€»")
    print(f"ğŸ¥¶ å¹³å‡å†·å¯åŠ¨: {avg_cold:.3f}s")
    print(f"ğŸ”¥ å¹³å‡ç¼“å­˜1: {avg_cached1:.3f}s")
    print(f"ğŸ”„ å¹³å‡ç¼“å­˜2: {avg_cached2:.3f}s")
    print(f"ğŸš€ å¹³å‡åŠ é€Ÿæ¯”: {avg_cold/avg_cached1:.2f}x / {avg_cold/avg_cached2:.2f}x")


def parse_args():
    parser = argparse.ArgumentParser(description="ç®€åŒ–ç‰ˆ LMCache æµ‹è¯•")
    parser.add_argument("--dataset-file", type=str, required=True,
                        help="æœ¬åœ°æ•°æ®é›†æ–‡ä»¶è·¯å¾„ (.json æˆ– .txt)")
    parser.add_argument("--model", type=str, default="facebook/opt-125m",
                        help="æ¨¡å‹åç§° (é»˜è®¤: facebook/opt-125m)")
    parser.add_argument("--port", type=int, default=8000,
                        help="vLLM ç«¯å£ (é»˜è®¤: 8000)")
    return parser.parse_args()


def main():
    args = parse_args()
    base_url = f"http://localhost:{args.port}/v1"

    print(f"ğŸ§ª LMCache æµ‹è¯• (æ¨¡å‹={args.model}, å®ä¾‹={base_url})")

    dataset = load_dataset_from
#!/usr/bin/env python3
"""
ä»æœ¬åœ° Hugging Face ç¼“å­˜åŠ è½½æ•°æ®é›†çš„ LMCache æµ‹è¯•è„šæœ¬
é¿å…ä½¿ç”¨ datasets åº“ï¼Œç›´æ¥ä»ç¼“å­˜æ–‡ä»¶è¯»å–æ•°æ®
"""

import requests
import json
import time
import argparse
import os
from pathlib import Path
from typing import List, Dict, Tuple


def find_hf_cache_dir() -> Path:
    """æŸ¥æ‰¾ Hugging Face ç¼“å­˜ç›®å½•"""
    hf_cache_home = os.environ.get("HF_HOME", Path.home() / ".cache/huggingface")
    cache_dir = Path(hf_cache_home) / "hub"
    return cache_dir


def find_dataset_cache(dataset_name: str) -> Path:
    """æŸ¥æ‰¾æ•°æ®é›†ç¼“å­˜ç›®å½•"""
    cache_dir = find_hf_cache_dir()
    print(f"ğŸ“ æœç´¢ç¼“å­˜ç›®å½•: {cache_dir}")
    
    # è½¬æ¢æ•°æ®é›†åç§°ä¸ºç¼“å­˜ç›®å½•å
    cache_name = dataset_name.replace("/", "--")
    
    for item in cache_dir.iterdir():
        if item.is_dir() and cache_name.lower() in item.name.lower():
            print(f"âœ… æ‰¾åˆ°ç¼“å­˜ç›®å½•: {item}")
            return item
    
    print(f"âŒ æœªæ‰¾åˆ°æ•°æ®é›†ç¼“å­˜: {dataset_name}")
    print(f"ğŸ’¡ è¯·å…ˆä¸‹è½½æ•°æ®é›†æˆ–æ£€æŸ¥ç¼“å­˜ç›®å½•")
    return None


def load_from_cache_file(cache_dir: Path, max_samples: int = 20) -> List[str]:
    """ä»ç¼“å­˜æ–‡ä»¶åŠ è½½æ•°æ®"""
    # ä¼˜å…ˆæŸ¥æ‰¾ snapshots ç›®å½•ä¸­çš„æ•°æ®æ–‡ä»¶
    snapshots_dir = cache_dir / "snapshots"
    data_files = []
    
    if snapshots_dir.exists():
        for pattern in ["*.json", "*.jsonl", "*.parquet"]:
            data_files.extend(snapshots_dir.rglob(pattern))
    
    # å¦‚æœ snapshots ä¸­æ²¡æœ‰æ‰¾åˆ°ï¼Œå†æŸ¥æ‰¾æ•´ä¸ªç›®å½•
    if not data_files:
        for pattern in ["*.json", "*.jsonl", "*.parquet"]:
            data_files.extend(cache_dir.rglob(pattern))
    
    if not data_files:
        print(f"âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
        return []
    
    # ä¼˜å…ˆé€‰æ‹© parquet æ–‡ä»¶ï¼Œç„¶åæ˜¯ jsonlï¼Œæœ€åæ˜¯ json
    data_files.sort(key=lambda x: (x.suffix != '.parquet', x.suffix != '.jsonl', x.suffix != '.json'))
    data_file = data_files[0]
    print(f"ğŸ“„ ä½¿ç”¨æ•°æ®æ–‡ä»¶: {data_file}")
    
    prompts = []
    try:
        if data_file.suffix == '.json':
            with open(data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    items = data
                elif isinstance(data, dict):
                    # å°è¯•æ‰¾åˆ°æ•°æ®åˆ—è¡¨
                    for key in ['train', 'data', 'examples']:
                        if key in data and isinstance(data[key], list):
                            items = data[key]
                            break
                    else:
                        items = [data]
                else:
                    items = [data]
        elif data_file.suffix == '.jsonl':
            items = []
            with open(data_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        items.append(json.loads(line))
        elif data_file.suffix == '.parquet':
            try:
                import pandas as pd
                df = pd.read_parquet(data_file)
                items = df.to_dict('records')
            except ImportError:
                print("âŒ éœ€è¦å®‰è£… pandas æ¥è¯»å– parquet æ–‡ä»¶: pip install pandas")
                return []
        else:
            print(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {data_file.suffix}")
            return []
        
        # æå– prompts
        for i, item in enumerate(items):
            if i >= max_samples:
                break
                
            if isinstance(item, str):
                prompt = item
            elif isinstance(item, dict):
                # æ ¹æ®æ•°æ®é›†ç±»å‹æå– prompt
                if 'text' in item:
                    prompt = item['text']
                elif 'question' in item and 'context' in item:
                    prompt = f"Context: {item['context']}\nQuestion: {item['question']}\nAnswer:"
                elif 'input' in item:
                    prompt = item['input']
                elif 'prompt' in item:
                    prompt = item['prompt']
                elif 'dialogue' in item:
                    prompt = f"Dialogue: {item['dialogue']}\nSummary:"
                elif 'article' in item:
                    prompt = f"Article: {item['article']}\nSummary:"
                else:
                    # å°è¯•ä½¿ç”¨ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸²å­—æ®µ
                    for key, value in item.items():
                        if isinstance(value, str) and len(value) > 50:
                            prompt = value
                            break
                    else:
                        continue
            else:
                continue
            
            # é™åˆ¶é•¿åº¦
            if len(prompt) > 2000:
                prompt = prompt[:2000] + "..."
            
            prompts.append(prompt)
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(prompts)} ä¸ªæ ·æœ¬")
        return prompts
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
        return []


def measure_ttft(base_url: str, model: str, prompt: str, tag: str) -> float:
    """æµ‹é‡ TTFT"""
    print(f"ğŸ“¤ {tag}...")
    start = time.perf_counter()
    response = requests.post(
        f"{base_url}/completions",
        json={
            "model": model,
            "prompt": prompt,
            "temperature": 0.0,
            "max_tokens": 20,
            "stream": False
        },
        headers={"Content-Type": "application/json"},
        timeout=60
    )
    response.raise_for_status()
    end = time.perf_counter()
    ttft = end - start
    print(f"ğŸ• TTFT = {ttft:.3f}s")
    return ttft


def test_dataset(base_url: str, model: str, dataset: List[str]):
    """æµ‹è¯•æ•°æ®é›†æ€§èƒ½"""
    results = []

    for i, prompt in enumerate(dataset, 1):
        print(f"\nğŸ“ Prompt {i}/{len(dataset)} (é•¿åº¦ {len(prompt)} å­—ç¬¦)")

        # å†·å¯åŠ¨
        ttft_cold = measure_ttft(base_url, model, prompt, "å†·å¯åŠ¨")
        # ç¬¬ä¸€æ¬¡ç¼“å­˜å¤ç”¨
        ttft_cached1 = measure_ttft(base_url, model, prompt, "ç¬¬ä¸€æ¬¡ç¼“å­˜å¤ç”¨")
        # ç¬¬äºŒæ¬¡ç¼“å­˜å¤ç”¨
        ttft_cached2 = measure_ttft(base_url, model, prompt, "ç¬¬äºŒæ¬¡ç¼“å­˜å¤ç”¨")

        print(f"ğŸš€ åŠ é€Ÿæ¯”: {ttft_cold/ttft_cached1:.2f}x, {ttft_cold/ttft_cached2:.2f}x")

        results.append((ttft_cold, ttft_cached1, ttft_cached2))

        time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«

    # ç»Ÿè®¡å¹³å‡
    avg_cold = sum(r[0] for r in results) / len(results)
    avg_cached1 = sum(r[1] for r in results) / len(results)
    avg_cached2 = sum(r[2] for r in results) / len(results)

    print("\nğŸ¯ æµ‹è¯•ç»“æœæ±‡æ€»")
    print(f"ğŸ¥¶ å¹³å‡å†·å¯åŠ¨: {avg_cold:.3f}s")
    print(f"ğŸ”¥ å¹³å‡ç¼“å­˜1: {avg_cached1:.3f}s")
    print(f"ğŸ”„ å¹³å‡ç¼“å­˜2: {avg_cached2:.3f}s")
    print(f"ğŸš€ å¹³å‡åŠ é€Ÿæ¯”: {avg_cold/avg_cached1:.2f}x / {avg_cold/avg_cached2:.2f}x")


def list_available_datasets():
    """åˆ—å‡ºå¯ç”¨çš„æ•°æ®é›†"""
    cache_dir = find_hf_cache_dir()
    print(f"ğŸ“‹ å¯ç”¨çš„ Hugging Face æ•°æ®é›†ç¼“å­˜:")
    print(f"ğŸ“ ç¼“å­˜ç›®å½•: {cache_dir}")
    
    if not cache_dir.exists():
        print("âŒ ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")
        return
    
    datasets = []
    for item in cache_dir.iterdir():
        if item.is_dir() and item.name.startswith("datasets--"):
            dataset_name = item.name.replace("datasets--", "").replace("--", "/")
            datasets.append(dataset_name)
    
    if datasets:
        for dataset in sorted(datasets):
            print(f"   ğŸ“Š {dataset}")
    else:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ•°æ®é›†ç¼“å­˜")


def parse_args():
    parser = argparse.ArgumentParser(description="ä» Hugging Face ç¼“å­˜åŠ è½½æ•°æ®é›†çš„ LMCache æµ‹è¯•")
    parser.add_argument("--dataset", type=str, help="Hugging Face æ•°æ®é›†åç§° (ä¾‹å¦‚: knkarthick/samsum)")
    parser.add_argument("--model", type=str, default="Qwen2.5-0.5B-Instruct",
                        help="æ¨¡å‹åç§° (é»˜è®¤: Qwen2.5-0.5B-Instruct)")
    parser.add_argument("--port", type=int, default=8002,
                        help="vLLM ç«¯å£ (é»˜è®¤: 8002)")
    parser.add_argument("--max-samples", type=int, default=10,
                        help="æœ€å¤§æ ·æœ¬æ•° (é»˜è®¤: 10)")
    parser.add_argument("--list", action="store_true",
                        help="åˆ—å‡ºå¯ç”¨çš„æ•°æ®é›†")
    return parser.parse_args()


def main():
    args = parse_args()
    
    if args.list:
        list_available_datasets()
        return
    
    if not args.dataset:
        print("âŒ è¯·æŒ‡å®šæ•°æ®é›†åç§°ï¼Œä½¿ç”¨ --dataset å‚æ•°")
        print("ğŸ’¡ ä½¿ç”¨ --list æŸ¥çœ‹å¯ç”¨æ•°æ®é›†")
        return
    
    base_url = f"http://localhost:{args.port}/v1"
    print(f"ğŸ§ª LMCache æµ‹è¯• (æ¨¡å‹={args.model}, å®ä¾‹={base_url})")
    print(f"ğŸ“Š æ•°æ®é›†: {args.dataset}")
    
    # æŸ¥æ‰¾æ•°æ®é›†ç¼“å­˜
    cache_dir = find_dataset_cache(args.dataset)
    if not cache_dir:
        return
    
    # åŠ è½½æ•°æ®
    dataset = load_from_cache_file(cache_dir, args.max_samples)
    if not dataset:
        return
    
    # æµ‹è¯•æ•°æ®é›†
    test_dataset(base_url, args.model, dataset)


if __name__ == "__main__":
    main()

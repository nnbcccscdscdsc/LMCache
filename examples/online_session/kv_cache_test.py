#!/usr/bin/env python3
"""
ä¸¥æ ¼çš„ KV Cache æµ‹è¯•
é€šè¿‡æ¯”è¾ƒç›¸åŒ prompt å’Œä¸åŒ prompt çš„æ€§èƒ½æ¥éªŒè¯ KV cache æ˜¯å¦çœŸçš„åœ¨å·¥ä½œ
æ”¯æŒå‘½ä»¤è¡ŒæŒ‡å®š vLLM å®ä¾‹ç«¯å£
"""

import requests
import json
import time
import random
import string
import argparse
from pathlib import Path

def rand_ascii(n: int) -> str:
    """ç”Ÿæˆéšæœº ASCII å­—ç¬¦ä¸²"""
    return "".join(random.choices(string.ascii_letters + string.digits, k=n))

def measure_ttft(base_url: str, model: str, prompt: str, description: str) -> tuple[float, dict]:
    """æµ‹é‡ TTFT"""
    print(f"ğŸ“¤ {description}...")
    start = time.perf_counter()
    
    try:
        response = requests.post(
            f"{base_url}/completions",
            json={
                "model": model,
                "prompt": prompt,
                "temperature": 0.0,
                "max_tokens": 20,
                "stream": False
            },
            headers={"Content-Type": "application/json"},
            timeout=60
        )
        response.raise_for_status()
        
        end = time.perf_counter()
        ttft = end - start
        
        result = response.json()
        generated_text = result['choices'][0]['text']
        usage = result.get('usage', {})
        
        print(f"ğŸ“¥ å“åº”: {generated_text[:30]}...")
        print(f"ğŸ“Š Tokens: {usage.get('prompt_tokens', 0)} -> {usage.get('completion_tokens', 0)}")
        
        return ttft, result
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
        raise

def test_single_instance_cache(base_url: str, model: str, prompt: str):
    """æµ‹è¯•å•å®ä¾‹çš„ç¼“å­˜åŠŸèƒ½"""
    results = []
    
    print(f"\nğŸ”„ === å•å®ä¾‹ LMCache æµ‹è¯• ===")
    print(f"ğŸ“„ æµ‹è¯• prompt: {len(prompt)} å­—ç¬¦")
    print(f"ğŸŒ æµ‹è¯•å®ä¾‹: {base_url}")
    
    # 1. å†·å¯åŠ¨
    print(f"\nğŸ¥¶ å†·å¯åŠ¨")
    ttft1, result1 = measure_ttft(base_url, model, prompt, "å†·å¯åŠ¨è¯·æ±‚")
    results.append(("cold_start", ttft1, result1))
    print(f"ğŸ• TTFT = {ttft1:.3f}s")
    
    # 2. ç›¸åŒ prompt (åº”è¯¥å‘½ä¸­ç¼“å­˜)
    print(f"\nğŸ”¥ ç›¸åŒ prompt (åº”è¯¥å‘½ä¸­ç¼“å­˜)")
    ttft2, result2 = measure_ttft(base_url, model, prompt, "ç¼“å­˜å‘½ä¸­è¯·æ±‚")
    results.append(("cached", ttft2, result2))
    print(f"ğŸ• TTFT = {ttft2:.3f}s")
    
    # 3. ä¸åŒ prompt (åº”è¯¥ä¸å‘½ä¸­ç¼“å­˜)
    different_prompt = "Document: " + rand_ascii(1000) + "\nSummary:"
    print(f"\nğŸ†• ä¸åŒ prompt (åº”è¯¥ä¸å‘½ä¸­ç¼“å­˜)")
    ttft3, result3 = measure_ttft(base_url, model, different_prompt, "ä¸åŒ prompt è¯·æ±‚")
    results.append(("different_prompt", ttft3, result3))
    print(f"ğŸ• TTFT = {ttft3:.3f}s")
    
    # 4. å†æ¬¡ç›¸åŒ prompt (åº”è¯¥å†æ¬¡å‘½ä¸­ç¼“å­˜)
    print(f"\nğŸ”„ å†æ¬¡ç›¸åŒ prompt (åº”è¯¥å†æ¬¡å‘½ä¸­ç¼“å­˜)")
    ttft4, result4 = measure_ttft(base_url, model, prompt, "å†æ¬¡ç›¸åŒ prompt è¯·æ±‚")
    results.append(("cached_again", ttft4, result4))
    print(f"ğŸ• TTFT = {ttft4:.3f}s")
    
    # 5. åˆ·æ–°ç¼“å­˜åæµ‹è¯•
    print(f"\nğŸ§¹ åˆ·æ–°ç¼“å­˜åæµ‹è¯•")
    flush_kv_cache(base_url, model)
    time.sleep(2)  # ç­‰å¾…ç¼“å­˜åˆ·æ–°
    ttft5, result5 = measure_ttft(base_url, model, prompt, "åˆ·æ–°ç¼“å­˜åè¯·æ±‚")
    results.append(("after_flush", ttft5, result5))
    print(f"ğŸ• TTFT = {ttft5:.3f}s")
    
    return results

def test_cross_instance_cache(base_urls, model, prompt):
    """æµ‹è¯•è·¨å®ä¾‹çš„ç¼“å­˜å…±äº«"""
    results = []
    
    print(f"\nğŸ”„ === è·¨å®ä¾‹ LMCache å…±äº«æµ‹è¯• ===")
    print(f"ğŸ“„ æµ‹è¯• prompt: {len(prompt)} å­—ç¬¦")
    print(f"ğŸŒ æµ‹è¯•å®ä¾‹æ•°é‡: {len(base_urls)}")
    
    # 1. åœ¨ç¬¬ä¸€ä¸ªå®ä¾‹ä¸Šå†·å¯åŠ¨
    print(f"\nğŸ¥¶ å®ä¾‹ 1 (ç«¯å£ {base_urls[0].split(':')[-1]}): å†·å¯åŠ¨")
    ttft1, result1 = measure_ttft(base_urls[0], model, prompt, "å†·å¯åŠ¨è¯·æ±‚")
    results.append(("instance_1_cold", ttft1, result1))
    print(f"ğŸ• TTFT = {ttft1:.3f}s")
    
    # 2. åœ¨ç¬¬ä¸€ä¸ªå®ä¾‹ä¸Šå†æ¬¡è¯·æ±‚ï¼ˆåº”è¯¥å‘½ä¸­ LMCacheï¼‰
    print(f"\nğŸ”¥ å®ä¾‹ 1 (ç«¯å£ {base_urls[0].split(':')[-1]}): ç›¸åŒ prompt")
    ttft1_cached, result1_cached = measure_ttft(base_urls[0], model, prompt, "ç¼“å­˜å‘½ä¸­è¯·æ±‚")
    results.append(("instance_1_cached", ttft1_cached, result1_cached))
    print(f"ğŸ• TTFT = {ttft1_cached:.3f}s")
    
    # 3. åœ¨å…¶ä»–å®ä¾‹ä¸Šè¯·æ±‚ç›¸åŒ promptï¼ˆæµ‹è¯•è·¨å®ä¾‹ LMCache å…±äº«ï¼‰
    for i, base_url in enumerate(base_urls[1:], 2):
        port = base_url.split(':')[-1]
        print(f"\nğŸ”„ å®ä¾‹ {i} (ç«¯å£ {port}): æµ‹è¯•è·¨å®ä¾‹ LMCache å…±äº«")
        ttft_cross, result_cross = measure_ttft(base_url, model, prompt, f"å®ä¾‹ {i} è·¨å®ä¾‹è¯·æ±‚")
        results.append((f"instance_{i}_cross", ttft_cross, result_cross))
        print(f"ğŸ• TTFT = {ttft_cross:.3f}s")
        
        # åˆ†æè·¨å®ä¾‹ç¼“å­˜æ•ˆæœ
        if ttft_cross < ttft1 * 0.8:  # å¦‚æœæ¯”å†·å¯åŠ¨å¿« 20% ä»¥ä¸Š
            print(f"âœ… å®ä¾‹ {i} æˆåŠŸå¤ç”¨äº†å®ä¾‹ 1 çš„ LMCacheï¼")
        elif ttft_cross < ttft1 * 1.1:  # å¦‚æœæ¥è¿‘å†·å¯åŠ¨æ—¶é—´
            print(f"ğŸŸ¡ å®ä¾‹ {i} å¯èƒ½æœ‰éƒ¨åˆ† LMCache å¤ç”¨")
        else:
            print(f"ğŸ”´ å®ä¾‹ {i} æ²¡æœ‰ LMCache å¤ç”¨")
    
    # 4. å›åˆ°ç¬¬ä¸€ä¸ªå®ä¾‹ï¼Œå†æ¬¡è¯·æ±‚ï¼ˆéªŒè¯ç¼“å­˜ä»ç„¶æœ‰æ•ˆï¼‰
    print(f"\nğŸ”„ å®ä¾‹ 1 (ç«¯å£ {base_urls[0].split(':')[-1]}): éªŒè¯ç¼“å­˜æŒä¹…æ€§")
    ttft1_verify, result1_verify = measure_ttft(base_urls[0], model, prompt, "éªŒè¯ç¼“å­˜æŒä¹…æ€§")
    results.append(("instance_1_verify", ttft1_verify, result1_verify))
    print(f"ğŸ• TTFT = {ttft1_verify:.3f}s")
    
    return results

def flush_kv_cache(base_url: str, model: str) -> None:
    """ä½¿ç”¨æ™ºèƒ½æˆªæ–­çš„æ¿€è¿›ç­–ç•¥åˆ·æ–° KV ç¼“å­˜"""
    print("ğŸ”„ æ™ºèƒ½æˆªæ–­ç­–ç•¥åˆ·æ–° KV ç¼“å­˜...")
    
    # ä½¿ç”¨ä¸ openai_chat_completion_client.py ç›¸åŒçš„å‚æ•°ï¼Œä½†ä¼šæ™ºèƒ½æˆªæ–­
    FILLER_LEN_CHARS = 100_000  # åŸå§‹é•¿åº¦
    NUM_FILLER_PROMPTS = 10      # å‘é€ 10 ä¸ªå¡«å……è¯·æ±‚
    
    # å®‰å…¨é•¿åº¦ï¼šç¡®ä¿åœ¨æ¨¡å‹é™åˆ¶å†…
    # facebook/opt-125m ä¸Šä¸‹æ–‡çª—å£æ˜¯ 2048 tokensï¼Œç•™å‡ºå®‰å…¨è¾¹è·
    SAFE_TOKEN_LIMIT = 600  # ä¿å®ˆä¼°è®¡ï¼Œçº¦ 1500 tokens
    
    for i in range(NUM_FILLER_PROMPTS):
        # ç”Ÿæˆè¶…é•¿ prompt
        long_prompt = f"Random filler text {i}: {rand_ascii(FILLER_LEN_CHARS)}"
        
        # æ™ºèƒ½æˆªæ–­ï¼šå¦‚æœ prompt å¤ªé•¿ï¼Œæˆªæ–­åˆ°å®‰å…¨é•¿åº¦
        if len(long_prompt) > SAFE_TOKEN_LIMIT * 4:  # å‡è®¾ 4 å­—ç¬¦ â‰ˆ 1 token
            safe_prompt = long_prompt[:SAFE_TOKEN_LIMIT * 4]
            print(f"  ğŸ“ æˆªæ–­ prompt {i+1} åˆ°å®‰å…¨é•¿åº¦: {len(safe_prompt)} å­—ç¬¦")
        else:
            safe_prompt = long_prompt
        
        try:
            response = requests.post(
                f"{base_url}/completions",
                json={
                    "model": model,
                    "prompt": safe_prompt,
                    "temperature": 0.0,
                    "max_tokens": 1,
                    "stream": False
                },
                headers={"Content-Type": "application/json"},
                timeout=60
            )
            response.raise_for_status()
            print(f"  âœ… å¡«å…… {i+1}/{NUM_FILLER_PROMPTS} å®Œæˆ")
        except Exception as e:
            print(f"  âš ï¸ å¡«å…… {i+1}/{NUM_FILLER_PROMPTS} å¤±è´¥: {e}")
            # å¦‚æœä»ç„¶å¤±è´¥ï¼Œå°è¯•æ›´çŸ­çš„ prompt
            try:
                short_prompt = f"Filler {i}: {rand_ascii(1000)}"
                response = requests.post(
                    f"{base_url}/completions",
                    json={
                        "model": model,
                        "prompt": short_prompt,
                        "temperature": 0.0,
                        "max_tokens": 1,
                        "stream": False
                    },
                    headers={"Content-Type": "application/json"},
                    timeout=30
                )
                response.raise_for_status()
                print(f"  âœ… å¡«å…… {i+1}/{NUM_FILLER_PROMPTS} å®Œæˆ (ä½¿ç”¨çŸ­ prompt)")
            except Exception as e2:
                print(f"  âŒ å¡«å…… {i+1}/{NUM_FILLER_PROMPTS} å®Œå…¨å¤±è´¥: {e2}")
    
    print("ğŸ”„ æ™ºèƒ½æˆªæ–­ç­–ç•¥ KV ç¼“å­˜åˆ·æ–°å®Œæˆ")

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="LMCache æµ‹è¯•è„šæœ¬")
    parser.add_argument(
        "--ports", 
        nargs="+", 
        type=int, 
        default=[8000],
        help="è¦æµ‹è¯•çš„ vLLM å®ä¾‹ç«¯å£åˆ—è¡¨ (é»˜è®¤: 8000)"
    )
    parser.add_argument(
        "--model", 
        type=str, 
        default="facebook/opt-125m",
        help="è¦æµ‹è¯•çš„æ¨¡å‹åç§° (é»˜è®¤: facebook/opt-125m)"
    )
    parser.add_argument(
        "--test-type",
        choices=["single", "cross"],
        default="single",
        help="æµ‹è¯•ç±»å‹: single(å•å®ä¾‹) æˆ– cross(è·¨å®ä¾‹) (é»˜è®¤: single)"
    )
    return parser.parse_args()

def main():
    args = parse_args()
    
    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°æ„å»º base_urls
    base_urls = [f"http://localhost:{port}/v1" for port in args.ports]
    
    model = args.model
    test_type = args.test_type
    
    print("ğŸ§ª LMCache æµ‹è¯•")
    print(f"ğŸ“ æµ‹è¯•ç±»å‹: {test_type}")
    print(f"ğŸ“ æµ‹è¯•å®ä¾‹: {len(base_urls)} ä¸ª")
    print(f"ğŸ¤– Model: {model}")
    print(f"ğŸŒ ç«¯å£: {args.ports}")
    print("=" * 80)
    
    # æ£€æŸ¥å®ä¾‹æ˜¯å¦å¯ç”¨
    print("ğŸ” æ£€æŸ¥ vLLM å®ä¾‹çŠ¶æ€...")
    available_instances = []
    for base_url in base_urls:
        try:
            response = requests.get(f"{base_url}/models", timeout=10)
            if response.status_code == 200:
                available_instances.append(base_url)
                port = base_url.split(':')[-1]
                print(f"âœ… å®ä¾‹ {port} å¯ç”¨")
            else:
                print(f"âŒ å®ä¾‹ {base_url.split(':')[-1]} å“åº”å¼‚å¸¸: {response.status_code}")
        except Exception as e:
            print(f"âŒ å®ä¾‹ {base_url.split(':')[-1]} ä¸å¯ç”¨: {e}")
    
    if len(available_instances) == 0:
        print(f"\nâŒ æ²¡æœ‰å¯ç”¨çš„ vLLM å®ä¾‹")
        print("ğŸ’¡ è¯·ç¡®ä¿è‡³å°‘å¯åŠ¨ä¸€ä¸ª vLLM å®ä¾‹")
        return 1
    
    print(f"\nâœ… å¯ç”¨å®ä¾‹: {len(available_instances)} ä¸ª")
    
    # åˆ›å»ºæµ‹è¯• prompt
    test_prompt = "Document: " + rand_ascii(1000) + "\nSummary:"
    print(f"ğŸ“„ æµ‹è¯• prompt é•¿åº¦: {len(test_prompt)} å­—ç¬¦")
    
    try:
        # æ ¹æ®æµ‹è¯•ç±»å‹æ‰§è¡Œä¸åŒçš„æµ‹è¯•
        if test_type == "single" or len(available_instances) == 1:
            print("\nğŸ”„ æ‰§è¡Œå•å®ä¾‹æµ‹è¯•...")
            results = test_single_instance_cache(available_instances[0], model, test_prompt)
        else:
            print("\nğŸ”„ æ‰§è¡Œè·¨å®ä¾‹æµ‹è¯•...")
            results = test_cross_instance_cache(available_instances, model, test_prompt)
        
        # åˆ†æç»“æœ
        print(f"\n\033[32mğŸ¯ === LMCache æµ‹è¯•åˆ†æç»“æœ ===")
        
        # æ‰¾åˆ°å†·å¯åŠ¨æ—¶é—´
        cold_start = None
        for test_type, ttft, _ in results:
            if "cold" in test_type:
                cold_start = ttft
                break
        
        if cold_start is None:
            print("âŒ æœªæ‰¾åˆ°å†·å¯åŠ¨æ•°æ®")
            return 1
        
        print(f"ğŸ¥¶ å†·å¯åŠ¨æ—¶é—´: {cold_start:.3f}s")
        
        # åˆ†ææ¯ä¸ªæµ‹è¯•çš„æ€§èƒ½
        for test_type, ttft, _ in results:
            if "cold" not in test_type:
                speedup = cold_start / ttft
                if speedup > 1.2:
                    print(f"âœ… {test_type}: {ttft:.3f}s (åŠ é€Ÿ: {speedup:.2f}x)")
                elif speedup > 1.1:
                    print(f"ğŸŸ¡ {test_type}: {ttft:.3f}s (åŠ é€Ÿ: {speedup:.2f}x)")
                else:
                    print(f"ğŸ”´ {test_type}: {ttft:.3f}s (åŠ é€Ÿ: {speedup:.2f}x)")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        out_path = Path(f"lmcache_test_results_{test_type}.jsonl")
        with out_path.open("w", encoding="utf-8") as f:
            for i, (test_type, ttft, result) in enumerate(results, 1):
                json.dump({
                    "test_index": i,
                    "test_type": test_type,
                    "ttft_seconds": ttft,
                    "usage": result.get('usage', {}),
                    "instance_port": test_type.split('_')[1] if '_' in test_type else "unknown"
                }, f)
                f.write("\n")
        
        print(f"\nğŸ“„ è¯¦ç»†ç»“æœä¿å­˜åˆ°: {out_path}")
        print("\033[0m")
        
        return 0
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return 1

if __name__ == "__main__":
    exit(main())

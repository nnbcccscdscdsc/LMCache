#!/usr/bin/env python3
"""
ä¸‹è½½å’Œé¢„å¤„ç† complexquestion_2WIKIMQA_1000 æ•°æ®é›†
ç”¨äº LMCache æ€§èƒ½æµ‹è¯•
"""

import json
import os
import sys
from pathlib import Path

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–åº“æ˜¯å¦å¯ç”¨"""
    print("ğŸ” æ£€æŸ¥ä¾èµ–åº“...")
    
    # æ£€æŸ¥ NumPy ç‰ˆæœ¬
    try:
        import numpy as np
        numpy_version = np.__version__
        print(f"âœ… NumPy: {numpy_version}")
        
        # æ£€æŸ¥ NumPy ç‰ˆæœ¬å…¼å®¹æ€§
        if numpy_version.startswith('2.'):
            print("âš ï¸  è­¦å‘Š: æ£€æµ‹åˆ° NumPy 2.x ç‰ˆæœ¬")
            print("ğŸ’¡ å»ºè®®: é™çº§åˆ° NumPy 1.x ä»¥é¿å…å…¼å®¹æ€§é—®é¢˜")
            print("   å‘½ä»¤: pip install 'numpy<2.0'")
    except ImportError:
        print("âŒ NumPy æœªå®‰è£…")
        return False
    
    # æ£€æŸ¥ datasets åº“
    try:
        from datasets import load_dataset
        print("âœ… datasets åº“å¯ç”¨")
        return True
    except ImportError as e:
        print(f"âŒ datasets åº“å¯¼å…¥å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·å®‰è£…: pip install datasets")
        return False
    except Exception as e:
        print(f"âŒ datasets åº“å­˜åœ¨ä½†æœ‰é—®é¢˜: {e}")
        if "NumPy" in str(e) or "array_api" in str(e):
            print("ğŸ”§ è¿™æ˜¯ NumPy å…¼å®¹æ€§é—®é¢˜")
            print("ğŸ’¡ è§£å†³æ–¹æ¡ˆ: pip install 'numpy<2.0'")
        return False

def download_complex_qa_dataset():
    """ä¸‹è½½å¤æ‚é—®ç­”æ•°æ®é›†"""
    print("ğŸ“¥ å¼€å§‹ä¸‹è½½ complexquestion_2WIKIMQA_1000 æ•°æ®é›†...")
    
    # é¦–å…ˆæ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        print("âŒ ä¾èµ–æ£€æŸ¥å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
        return None
    
    try:
        # å¯¼å…¥ datasets åº“
        from datasets import load_dataset
        
        # è®¾ç½® Hugging Face é…ç½®
        print("âš™ï¸ é…ç½® Hugging Face è¿æ¥...")
        
        # å°è¯•è®¾ç½®é•œåƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            os.environ.setdefault('HF_ENDPOINT', 'https://huggingface.co')
            print(f"ğŸŒ ä½¿ç”¨ç«¯ç‚¹: {os.environ.get('HF_ENDPOINT')}")
        except:
            pass
        
        # ä¸‹è½½æ•°æ®é›†
        print("ğŸ”„ æ­£åœ¨ä» Hugging Face ä¸‹è½½æ•°æ®é›†...")
        dataset = load_dataset('presencesw/complexquestion_2WIKIMQA_1000')
        
        print(f"âœ… æ•°æ®é›†ä¸‹è½½æˆåŠŸ!")
        print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
        print(f"   - è®­ç»ƒé›†å¤§å°: {len(dataset['train'])} è¡Œ")
        print(f"   - åˆ—å: {dataset['train'].column_names}")
        
        # æ˜¾ç¤ºç¤ºä¾‹æ•°æ®
        print(f"\nğŸ“ ç¤ºä¾‹æ•°æ®:")
        example = dataset['train'][0]
        for key, value in example.items():
            if isinstance(value, str) and len(value) > 100:
                print(f"   {key}: {value[:100]}...")
            else:
                print(f"   {key}: {value}")
        
        return dataset
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†ä¸‹è½½å¤±è´¥: {e}")
        
        # æ ¹æ®é”™è¯¯ç±»å‹æä¾›å…·ä½“çš„è¯Šæ–­
        error_str = str(e).lower()
        
        if "numpy" in error_str or "array_api" in error_str:
            print(f"\nğŸ”§ è¿™æ˜¯ NumPy å…¼å®¹æ€§é—®é¢˜!")
            print(f"ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
            print(f"   1. é™çº§ NumPy: pip install 'numpy<2.0'")
            print(f"   2. æˆ–è€…å‡çº§ç›¸å…³æ¨¡å—: pip install --upgrade pandas numexpr bottleneck")
            print(f"   3. æˆ–è€…ä½¿ç”¨ conda ç¯å¢ƒ: conda install numpy pandas")
        
        elif "connection" in error_str or "timeout" in error_str:
            print(f"\nğŸ”§ è¿™æ˜¯ç½‘ç»œè¿æ¥é—®é¢˜!")
            print(f"ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
            print(f"   1. æ£€æŸ¥ä»£ç†è®¾ç½®: env | grep -i proxy")
            print(f"   2. è®¾ç½®æ­£ç¡®çš„ä»£ç†: export http_proxy='http://ä»£ç†åœ°å€:ç«¯å£'")
            print(f"   3. æˆ–è€…ç¦ç”¨ä»£ç†: export http_proxy='' && export https_proxy=''")
        
        elif "not found" in error_str or "doesn't exist" in error_str:
            print(f"\nğŸ”§ è¿™æ˜¯æ•°æ®é›†è·¯å¾„é—®é¢˜!")
            print(f"ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
            print(f"   1. æ£€æŸ¥æ•°æ®é›†åç§°æ˜¯å¦æ­£ç¡®")
            print(f"   2. ç¡®è®¤æ•°æ®é›†æ˜¯å¦å…¬å¼€å¯ç”¨")
        
        else:
            print(f"\nğŸ”§ æœªçŸ¥é”™è¯¯ç±»å‹")
            print(f"ğŸ’¡ å»ºè®®æ“ä½œ:")
            print(f"   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print(f"   2. æ£€æŸ¥ä»£ç†è®¾ç½®")
            print(f"   3. å°è¯•ä½¿ç”¨ VPN")
        
        return None

def extract_prompts_for_testing(dataset, max_prompts=20):
    """æå–ç”¨äºæµ‹è¯•çš„ prompts"""
    print(f"\nï¿½ï¿½ æå–æµ‹è¯• prompts (æœ€å¤š {max_prompts} ä¸ª)...")
    
    prompts = []
    train_data = dataset['train']
    
    for i in range(min(max_prompts, len(train_data))):
        item = train_data[i]
        
        # æ„å»ºæµ‹è¯• prompt
        # ä¼˜å…ˆä½¿ç”¨ user_promptï¼Œå› ä¸ºå®ƒåŒ…å«äº†å®Œæ•´çš„ä¸Šä¸‹æ–‡å’Œé—®é¢˜
        if 'user_prompt' in item and item['user_prompt']:
            prompt = item['user_prompt']
        elif 'complex_question' in item and item['complex_question']:
            # å¦‚æœæ²¡æœ‰ user_promptï¼Œä½¿ç”¨ complex_question æ„å»ºç®€å• prompt
            prompt = f"Question: {item['complex_question']}\nAnswer:"
        else:
            continue
        
        # é™åˆ¶ prompt é•¿åº¦ï¼Œé¿å…è¶…å‡ºæ¨¡å‹ä¸Šä¸‹æ–‡çª—å£
        if len(prompt) > 1500:  # facebook/opt-125m ä¸Šä¸‹æ–‡çª—å£æ˜¯ 2048 tokensï¼Œç•™å‡ºå®‰å…¨è¾¹è·
            prompt = prompt[:1500] + "..."
        
        prompts.append({
            "index": i + 1,
            "original_length": len(item.get('user_prompt', '')),
            "truncated_length": len(prompt),
            "prompt": prompt,
            "complex_question": item.get('complex_question', ''),
            "answer": item.get('gpt_answer', ''),
            "entities": item.get('entities', []),
            "triplets": item.get('triplets', [])
        })
    
    print(f"âœ… æˆåŠŸæå– {len(prompts)} ä¸ªæµ‹è¯• prompts")
    return prompts

def save_prompts_to_files(prompts):
    """ä¿å­˜ prompts åˆ°ä¸åŒæ ¼å¼çš„æ–‡ä»¶"""
    print(f"\nğŸ’¾ ä¿å­˜ prompts åˆ°æ–‡ä»¶...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("dataset_prompts")
    output_dir.mkdir(exist_ok=True)
    
    # 1. ä¿å­˜ä¸º JSON æ ¼å¼
    json_file = output_dir / "complex_qa_prompts.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump({
            "dataset_name": "complexquestion_2WIKIMQA_1000",
            "source": "https://huggingface.co/datasets/presencesw/complexquestion_2WIKIMQA_1000",
            "description": "1000ä¸ªå¤æ‚é—®ç­”å¯¹ï¼ŒåŒ…å«å¤šæ­¥æ¨ç†é—®é¢˜",
            "total_prompts": len(prompts),
            "prompts": prompts
        }, f, ensure_ascii=False, indent=2)
    print(f"âœ… JSON æ ¼å¼ä¿å­˜åˆ°: {json_file}")
    
    # 2. ä¿å­˜ä¸º TXT æ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ª promptï¼‰
    txt_file = output_dir / "complex_qa_prompts.txt"
    with open(txt_file, 'w', encoding='utf-8') as f:
        for prompt_data in prompts:
            f.write(prompt_data['prompt'] + '\n')
    print(f"âœ… TXT æ ¼å¼ä¿å­˜åˆ°: {txt_file}")
    
    # 3. ä¿å­˜ä¸º JSONL æ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼‰
    jsonl_file = output_dir / "complex_qa_prompts.jsonl"
    with open(jsonl_file, 'w', encoding='utf-8') as f:
        for prompt_data in prompts:
            json.dump(prompt_data, f, ensure_ascii=False)
            f.write('\n')
    print(f"âœ… JSONL æ ¼å¼ä¿å­˜åˆ°: {jsonl_file}")
    
    return output_dir

def create_test_config():
    """åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶"""
    print(f"\nâš™ï¸ åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶...")
    
    config = {
        "dataset_info": {
            "name": "complexquestion_2WIKIMQA_1000",
            "source": "https://huggingface.co/datasets/presencesw/complexquestion_2WIKIMQA_1000",
            "description": "1000ä¸ªå¤æ‚é—®ç­”å¯¹ï¼ŒåŒ…å«å¤šæ­¥æ¨ç†é—®é¢˜",
            "note": "ä» Hugging Face ç›´æ¥ä¸‹è½½çš„çœŸå®æ•°æ®é›†"
        },
        "test_config": {
            "max_prompts": 20,
            "max_prompt_length": 1500,
            "model": "facebook/opt-125m",
            "test_types": ["cold_start", "cached_first", "cached_second"]
        },
        "usage_instructions": {
            "json_format": "ä½¿ç”¨ --dataset custom --dataset-file dataset_prompts/complex_qa_prompts.json",
            "txt_format": "ä½¿ç”¨ --dataset custom --dataset-file dataset_prompts/complex_qa_prompts.txt",
            "jsonl_format": "ä½¿ç”¨ --dataset custom --dataset-file dataset_prompts/complex_qa_prompts.jsonl"
        },
        "dataset_features": [
            "complex_question - å¤æ‚é—®é¢˜",
            "user_prompt - ç”¨æˆ·æç¤ºï¼ˆåŒ…å«ä¸Šä¸‹æ–‡ï¼‰",
            "gpt_answer - GPT ç­”æ¡ˆ",
            "entities - å®ä½“åºåˆ—",
            "triplets - ä¸‰å…ƒç»„åºåˆ—"
        ]
    }
    
    config_file = Path("dataset_prompts/test_config.json")
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æµ‹è¯•é…ç½®æ–‡ä»¶ä¿å­˜åˆ°: {config_file}")
    return config_file

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª Complex QA æ•°æ®é›†ä¸‹è½½å·¥å…·")
    print("=" * 60)
    print("ğŸ“¥ ç›®æ ‡: presencesw/complexquestion_2WIKIMQA_1000")
    print("ğŸŒ æ¥æº: https://huggingface.co/datasets/presencesw/complexquestion_2WIKIMQA_1000")
    print("=" * 60)
    
    # 1. ä¸‹è½½æ•°æ®é›†
    dataset = download_complex_qa_dataset()
    if dataset is None:
        print("\nâŒ æ— æ³•ä¸‹è½½æ•°æ®é›†")
        print("ğŸ’¡ è¯·æ ¹æ®ä¸Šé¢çš„é”™è¯¯è¯Šæ–­ä¿¡æ¯è§£å†³é—®é¢˜")
        return 1
    
    # 2. æå–æµ‹è¯• prompts
    prompts = extract_prompts_for_testing(dataset, max_prompts=20)
    if not prompts:
        print("âŒ æ²¡æœ‰æå–åˆ°æœ‰æ•ˆçš„ prompts")
        return 1
    
    # 3. ä¿å­˜åˆ°æ–‡ä»¶
    output_dir = save_prompts_to_files(prompts)
    
    # 4. åˆ›å»ºæµ‹è¯•é…ç½®
    config_file = create_test_config()
    
    # 5. æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
    print(f"\n\033[32mğŸ¯ === ä½¿ç”¨è¯´æ˜ ===")
    print(f"æ•°æ®é›†ä¸‹è½½å’Œé¢„å¤„ç†å®Œæˆ!")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ“Š å¯ç”¨ prompts: {len(prompts)} ä¸ª")
    print(f"ğŸ“„ åŸå§‹æ•°æ®é›†å¤§å°: {len(dataset['train'])} è¡Œ")
    print(f"\nğŸš€ è¿è¡Œæµ‹è¯•:")
    print(f"cd /home/limingjie/LMJWork/CacheBlend/LMCache/examples/online_session")
    print(f"python kv_cache_test_dateset.py --ports 8001 --dataset custom --dataset-file dataset_prompts/complex_qa_prompts.json")
    print(f"\næˆ–è€…ä½¿ç”¨ TXT æ ¼å¼:")
    print(f"python kv_cache_test_dateset.py --ports 8001 --dataset custom --dataset-file dataset_prompts/complex_qa_prompts.txt")
    print(f"\nğŸ“ æ•°æ®é›†ç‰¹ç‚¹:")
    print(f"   - çœŸå®çš„å¤æ‚æ¨ç†é—®é¢˜")
    print(f"   - åŒ…å«å®Œæ•´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯")
    print(f"   - æ¶µç›–å¤šä¸ªé¢†åŸŸçš„é—®ç­”å¯¹")
    print(f"   - ç»è¿‡é•¿åº¦ä¼˜åŒ–ï¼Œé€‚åˆæµ‹è¯•æ¨¡å‹")
    print("\033[0m")
    
    return 0

if __name__ == "__main__":
    exit(main())

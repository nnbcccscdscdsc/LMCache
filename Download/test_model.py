#!/usr/bin/env python3
"""
é€šç”¨ HuggingFace æ¨¡å‹æµ‹è¯•è„šæœ¬
æ”¯æŒæœ¬åœ°æ¨¡å‹ / HuggingFace Hub æ¨¡å‹
ç”¨æ³•:
    python test_model.py meta-llama/Llama-2-7b-chat-hf --cpu
    python test_model.py Qwen/Qwen2.5-1.5B-Instruct --prompt "ä½ å¥½"
    python test_model.py /path/to/local/model --local
    python test_model.py --list  # åˆ—å‡ºæœ¬åœ°å¯ç”¨æ¨¡å‹
    python test_model.py ~/.cache/huggingface/hub/models--Yukang--LongAlpaca-70B-16k/snapshots/594d3c7ba4fa3ea0720b9918820ef73dfcc5ab9b --local --cpu --prompt "Hello" --max-tokens 10
    python test_model.py ~/.cache/huggingface/hub/models--01-ai--Yi-34B-200K/snapshots/e0ae1afac6b69f604556efd441ab4befafb2a835 --local --cpu --prompt "Hello" --max-tokens 20
    python test_model.py ~/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/ec5deb64f2c6e6fa90c1abf74a91d5c93a9669ca --local --cpu --prompt "Hello" --max-tokens 20
"""

import argparse
import os
import sys
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch


def test_model(model_path: str, prompt: str, max_tokens: int = 128, use_gpu: bool = True, local_only: bool = False):
    """
    é€šç”¨æ¨¡å‹æµ‹è¯•å‡½æ•°
    Args:
        model_path: æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°è·¯å¾„æˆ– HuggingFace Hub åç§°ï¼‰
        prompt: è¾“å…¥æç¤º
        max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        use_gpu: æ˜¯å¦ä½¿ç”¨GPU
        local_only: æ˜¯å¦åªä½¿ç”¨æœ¬åœ°æ–‡ä»¶
    """
    print(f"ğŸ” æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
    
    # æ”¹è¿›çš„è·¯å¾„éªŒè¯é€»è¾‘
    if local_only or os.path.exists(model_path):
        # æœ¬åœ°è·¯å¾„éªŒè¯
        if not os.path.exists(model_path):
            print(f"âŒ æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            return False
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯snapshotsè·¯å¾„ï¼Œå¦‚æœæ˜¯åˆ™ç›´æ¥ä½¿ç”¨ï¼ˆä¸è½¬æ¢ï¼‰
        if "snapshots" in model_path:
            print(f"ğŸ“ æ£€æµ‹åˆ° snapshots è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨: {model_path}")
            # ä¸è½¬æ¢è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨ snapshots è·¯å¾„
    else:
        # HuggingFace Hub æ¨¡å‹åç§°éªŒè¯
        if "/" not in model_path:
            print(f"âš ï¸  è­¦å‘Š: æ¨¡å‹åç§° '{model_path}' å¯èƒ½ä¸æ˜¯æœ‰æ•ˆçš„ HuggingFace Hub æ ¼å¼")
            print("ğŸ’¡ å»ºè®®ä½¿ç”¨æ ¼å¼: ç”¨æˆ·å/æ¨¡å‹å (ä¾‹å¦‚: Qwen/Qwen2.5-1.5B-Instruct)")

    try:
        # åŠ è½½ tokenizer
        print("ğŸ“ åŠ è½½ tokenizer...")
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True,
                local_files_only=local_only
            )
        except Exception as tokenizer_error:
            print(f"âš ï¸  tokenizer åŠ è½½å¤±è´¥: {tokenizer_error}")
            print("ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨ tokenizer...")
            # å¯¹äºæœ‰é—®é¢˜çš„æ¨¡å‹ï¼Œå°è¯•ä½¿ç”¨ Llama tokenizer
            try:
                from transformers import LlamaTokenizer
                # ç¡®ä¿ model_path æ˜¯å­—ç¬¦ä¸²
                if not isinstance(model_path, str):
                    model_path = str(model_path)
                tokenizer = LlamaTokenizer.from_pretrained(
                    model_path,
                    trust_remote_code=True,
                    local_files_only=local_only
                )
                print("âœ… ä½¿ç”¨ Llama tokenizer æˆåŠŸ")
            except Exception as llama_error:
                print(f"âŒ Llama tokenizer ä¹Ÿå¤±è´¥: {llama_error}")
                # å°è¯•ä½¿ç”¨ GPT2 tokenizer ä½œä¸ºæœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ
                try:
                    from transformers import GPT2Tokenizer
                    print("ğŸ”„ å°è¯•ä½¿ç”¨ GPT2 tokenizer...")
                    tokenizer = GPT2Tokenizer.from_pretrained(
                        model_path,
                        trust_remote_code=True,
                        local_files_only=local_only
                    )
                    print("âœ… ä½¿ç”¨ GPT2 tokenizer æˆåŠŸ")
                except Exception as gpt2_error:
                    print(f"âŒ GPT2 tokenizer ä¹Ÿå¤±è´¥: {gpt2_error}")
                    raise tokenizer_error

        # è®¾ç½® pad_tokenï¼ˆå…¼å®¹å¤§å¤šæ•°æ¨¡å‹ï¼‰
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            print("ğŸ”§ è®¾ç½® pad_token ä¸º eos_token")

        # æ£€æŸ¥ GPU/CPU
        if use_gpu and torch.cuda.is_available():
            print("ğŸš€ ä½¿ç”¨ GPU æ¨¡å¼")
            device_map = "auto"
            torch_dtype = torch.float16
        else:
            print("ğŸ’» ä½¿ç”¨ CPU æ¨¡å¼")
            device_map = "cpu"
            torch_dtype = torch.float32

        # åŠ è½½æ¨¡å‹
        print("ğŸ¤– åŠ è½½æ¨¡å‹...")
        try:
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                trust_remote_code=True,
                torch_dtype=torch_dtype,
                device_map=device_map,
                low_cpu_mem_usage=True,
                local_files_only=local_only
            )
        except Exception as model_error:
            print(f"âš ï¸  æ¨¡å‹åŠ è½½å¤±è´¥: {model_error}")
            print("ğŸ”„ å°è¯•ä½¿ç”¨ Llama æ¨¡å‹æ¶æ„...")
            try:
                from transformers import LlamaForCausalLM
                model = LlamaForCausalLM.from_pretrained(
                    model_path,
                    trust_remote_code=True,
                    torch_dtype=torch_dtype,
                    device_map=device_map,
                    low_cpu_mem_usage=True,
                    local_files_only=local_only
                )
                print("âœ… ä½¿ç”¨ Llama æ¨¡å‹æ¶æ„æˆåŠŸ")
            except Exception as llama_model_error:
                print(f"âŒ Llama æ¨¡å‹æ¶æ„ä¹Ÿå¤±è´¥: {llama_model_error}")
                raise model_error

        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå¼€å§‹ç”Ÿæˆ...")

        # ç¼–ç è¾“å…¥
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        # ç”Ÿæˆæ–‡æœ¬ï¼ˆä¼˜åŒ–çš„å‚æ•°ï¼‰
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.8,  # ç¨å¾®é™ä½top_pæé«˜è´¨é‡
                top_k=50,
                repetition_penalty=1.05,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                use_cache=True,
                output_scores=False,
                return_dict_in_generate=False
            )

        # è§£ç è¾“å‡º
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        print("ğŸ¤– æ¨¡å‹è¾“å‡º:")
        print("=" * 50)
        print(generated_text)
        print("=" * 50)

        return True

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False


def list_local_models():
    """åˆ—å‡ºæœ¬åœ°å¯ç”¨çš„æ¨¡å‹"""
    cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
    local_models = []
    
    if os.path.exists(cache_dir):
        for item in os.listdir(cache_dir):
            if item.startswith("models--"):
                model_path = os.path.join(cache_dir, item)
                if os.path.isdir(model_path):
                    # æå–æ¨¡å‹åç§°
                    model_name = item.replace("models--", "").replace("--", "/")
                    local_models.append((model_name, model_path))
    
    if local_models:
        print("ğŸ“‹ æœ¬åœ°å¯ç”¨çš„æ¨¡å‹:")
        for i, (model_name, model_path) in enumerate(local_models, 1):
            print(f"  {i}. {model_name}")
            print(f"     è·¯å¾„: {model_path}")
    else:
        print("âŒ æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹")
        print("ğŸ’¡ æç¤º: ä½¿ç”¨ HuggingFace Hub æ¨¡å‹åç§°ä¼šè‡ªåŠ¨ä¸‹è½½åˆ°æœ¬åœ°")
    
    return local_models


def main():
    parser = argparse.ArgumentParser(description="é€šç”¨ HuggingFace æ¨¡å‹æµ‹è¯•è„šæœ¬")
    parser.add_argument("model_name", nargs="?", help="æ¨¡å‹åç§°æˆ–æœ¬åœ°è·¯å¾„")
    parser.add_argument("--prompt", default="ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±", help="è¾“å…¥æç¤º (é»˜è®¤: ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±)")
    parser.add_argument("--max-tokens", type=int, default=128, help="æœ€å¤§ç”Ÿæˆ token æ•° (é»˜è®¤: 128)")
    parser.add_argument("--cpu", action="store_true", help="å¼ºåˆ¶ä½¿ç”¨ CPU æ¨¡å¼")
    parser.add_argument("--local", action="store_true", help="å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶æ¨¡å¼")
    parser.add_argument("--list", action="store_true", help="åˆ—å‡ºæœ¬åœ°å¯ç”¨çš„æ¨¡å‹")

    args = parser.parse_args()
    
    # å¤„ç† --list å‚æ•°
    if args.list:
        list_local_models()
        return
    
    # æ£€æŸ¥æ˜¯å¦æä¾›äº†æ¨¡å‹åç§°
    if not args.model_name:
        print("âŒ è¯·æä¾›æ¨¡å‹åç§°")
        print("ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:")
        print("  python test_model.py Qwen/Qwen2.5-1.5B-Instruct --cpu")
        print("  python test_model.py /path/to/local/model --local")
        print("  python test_model.py --list  # æŸ¥çœ‹æœ¬åœ°å¯ç”¨æ¨¡å‹")
        return

    # å¦‚æœä½¿ç”¨ --local å‚æ•°ï¼Œå°† HuggingFace Hub åç§°è½¬æ¢ä¸ºæœ¬åœ°è·¯å¾„
    model_path = args.model_name
    if args.local and "/" in args.model_name and not os.path.exists(args.model_name):
        # å°† HuggingFace Hub åç§°è½¬æ¢ä¸ºæœ¬åœ°ç¼“å­˜è·¯å¾„
        cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
        model_cache_name = f"models--{args.model_name.replace('/', '--')}"
        model_path = os.path.join(cache_dir, model_cache_name)
        print(f"ğŸ”„ è½¬æ¢ HuggingFace Hub åç§°ä¸ºæœ¬åœ°è·¯å¾„: {model_path}")

    success = test_model(
        model_path,
        args.prompt,
        max_tokens=args.max_tokens,
        use_gpu=not args.cpu,
        local_only=args.local
    )

    if success:
        print("ğŸ‰ æ¨¡å‹æµ‹è¯•æˆåŠŸï¼")
    else:
        print("ğŸ’¥ æ¨¡å‹æµ‹è¯•å¤±è´¥ï¼")
        print("ğŸ’¡ æ•…éšœæ’é™¤å»ºè®®:")
        print("  1. æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®")
        print("  2. ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸ï¼ˆå¦‚æœä½¿ç”¨ HuggingFace Hubï¼‰")
        print("  3. æ£€æŸ¥æœ¬åœ°è·¯å¾„æ˜¯å¦å­˜åœ¨ï¼ˆå¦‚æœä½¿ç”¨ --localï¼‰")
        print("  4. ä½¿ç”¨ --list æŸ¥çœ‹æœ¬åœ°å¯ç”¨æ¨¡å‹")
        sys.exit(1)


if __name__ == "__main__":
    main()

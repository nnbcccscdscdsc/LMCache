#!/usr/bin/env python3
"""
ä¸“é—¨ç”¨äºæµ‹è¯•Qwenæ¨¡å‹çš„è„šæœ¬
æ”¯æŒQwen2.5ç³»åˆ—æ¨¡å‹çš„æœ¬åœ°æµ‹è¯•
ç”¨æ³•:
    python test_qwen_model.py <model_name> --cpu
    python test_qwen_model.py 1.5B --cpu
"""

import argparse
import os
import sys
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

def test_qwen_model(model_path: str, prompt: str, max_tokens: int = 128, use_gpu: bool = True):
    """
    æµ‹è¯•Qwenæ¨¡å‹
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„ï¼ˆå¯ä»¥æ˜¯ä¸»ç›®å½•æˆ–snapshotsè·¯å¾„ï¼‰
        prompt: è¾“å…¥æç¤º
        max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        use_gpu: æ˜¯å¦ä½¿ç”¨GPU
    """
    print(f"ğŸ” æ­£åœ¨åŠ è½½Qwenæ¨¡å‹: {model_path}")
    
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return False
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯snapshotsè·¯å¾„ï¼Œå¦‚æœæ˜¯åˆ™ä½¿ç”¨ä¸»ç›®å½•
    if "snapshots" in model_path:
        # ä»snapshotsè·¯å¾„æå–ä¸»ç›®å½•è·¯å¾„
        main_path = model_path.split("/snapshots/")[0]
        if os.path.exists(main_path):
            model_path = main_path
            print(f"ğŸ”„ ä½¿ç”¨ä¸»ç›®å½•è·¯å¾„: {model_path}")
        else:
            print(f"âŒ ä¸»ç›®å½•è·¯å¾„ä¸å­˜åœ¨: {main_path}")
            return False
    
    try:
        # åŠ è½½tokenizer
        print("ğŸ“ åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_path, 
            trust_remote_code=True,
            local_files_only=True
        )
        
        # è®¾ç½®pad_tokenï¼ˆQwenæ¨¡å‹ç‰¹æœ‰ï¼‰
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            print("ğŸ”§ è®¾ç½®pad_tokenä¸ºeos_token")
        
        # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
        if use_gpu and torch.cuda.is_available():
            print("ğŸš€ ä½¿ç”¨GPUæ¨¡å¼")
            device_map = "auto"
            torch_dtype = torch.float16
        else:
            print("ğŸ’» ä½¿ç”¨CPUæ¨¡å¼")
            device_map = "cpu"
            torch_dtype = torch.float32
        
        # åŠ è½½æ¨¡å‹ï¼ˆQwenæ¨¡å‹ç‰¹æœ‰é…ç½®ï¼‰
        print("ğŸ¤– åŠ è½½æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch_dtype,
            device_map=device_map,
            low_cpu_mem_usage=True,
            local_files_only=True
        )
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå¼€å§‹ç”Ÿæˆ...")
        
        # ç¼–ç è¾“å…¥
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        # ç”Ÿæˆæ–‡æœ¬ï¼ˆQwenæ¨¡å‹ä¼˜åŒ–å‚æ•°ï¼‰
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,           # Qwenæ¨¡å‹æ¨èæ¸©åº¦
                top_p=0.8,                # Qwenæ¨¡å‹æ¨ètop_p
                top_k=50,                 # Qwenæ¨¡å‹æ¨ètop_k
                repetition_penalty=1.05,   # è½»å¾®é‡å¤æƒ©ç½š
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                use_cache=True,           # Qwenæ¨¡å‹æ”¯æŒç¼“å­˜
                output_scores=False,
                return_dict_in_generate=False
            )
        
        # è§£ç è¾“å‡º
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        print("ğŸ¤– Qwenæ¨¡å‹è¾“å‡º:")
        print("=" * 50)
        print(generated_text)
        print("=" * 50)
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def list_qwen_models():
    """åˆ—å‡ºæœ¬åœ°å¯ç”¨çš„Qwenæ¨¡å‹"""
    cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
    qwen_models = []
    
    if os.path.exists(cache_dir):
        for item in os.listdir(cache_dir):
            if item.startswith("models--Qwen"):
                model_path = os.path.join(cache_dir, item)
                if os.path.isdir(model_path):
                    qwen_models.append(model_path)
    
    if qwen_models:
        print("ğŸ“‹ æœ¬åœ°å¯ç”¨çš„Qwenæ¨¡å‹:")
        for i, model in enumerate(qwen_models, 1):
            print(f"  {i}. {model}")
    else:
        print("âŒ æœªæ‰¾åˆ°æœ¬åœ°Qwenæ¨¡å‹")
    
    return qwen_models

def main():
    parser = argparse.ArgumentParser(description="æµ‹è¯•Qwenæ¨¡å‹")
    parser.add_argument("model_name", nargs="?")
    parser.add_argument("--prompt", default="ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±", help="è¾“å…¥æç¤º (é»˜è®¤: ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±)")
    parser.add_argument("--max-tokens", type=int, default=128, help="æœ€å¤§ç”Ÿæˆtokenæ•° (é»˜è®¤: 128)")
    parser.add_argument("--cpu", action="store_true", help="å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼")
    parser.add_argument("--list", action="store_true", help="åˆ—å‡ºæœ¬åœ°å¯ç”¨çš„Qwenæ¨¡å‹")
    
    args = parser.parse_args()
    
    if args.list:
        list_qwen_models()
        return
    
    if not args.model_name:
        print("âŒ è¯·æä¾›æ¨¡å‹åç§°")
        #ç›®å‰æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
        for model in qwen_models:
            print(model)
        print("ğŸ’¡ ä½¿ç”¨ --list æŸ¥çœ‹å®Œæ•´çš„æ¨¡å‹è·¯å¾„")
        return
    
    # æ ¹æ®æ¨¡å‹åç§°æ„å»ºè·¯å¾„
    model_path = f"/home/limingjie/.cache/huggingface/hub/models--Qwen--Qwen2.5-{args.model_name}-Instruct"
    
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return
    
    success = test_qwen_model(
        model_path, 
        args.prompt, 
        max_tokens=args.max_tokens,
        use_gpu=not args.cpu
    )
    
    if success:
        print("ğŸ‰ Qwenæ¨¡å‹æµ‹è¯•æˆåŠŸï¼")
    else:
        print("ğŸ’¥ Qwenæ¨¡å‹æµ‹è¯•å¤±è´¥ï¼")
        sys.exit(1)

if __name__ == "__main__":
    main()
